{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3~3.6 네트워크 모델 작성\n",
    "\n",
    "- PSPNet 네트워크 모델과 순전파 forward 함수를 작성합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 학습 목표\n",
    "\n",
    "1.\tPSPNet 네트워크 구조를 모듈 단위로 이해한다\n",
    "2.\tPSPNet를 구성하는 각 모듈의 역할을 이해한다\n",
    "3.\tPSPNet 네트워크 클래스의 구현을 이해한다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 학습 목표\n",
    "\n",
    "1. Feature 모듈의 서브 네트워크 구성을 이해한다\n",
    "2. 서브 네트워크 FeatureMap_convolution을 구현할 수 있다\n",
    "3. Residual Block을 이해한다\n",
    "4. Dilated Convolution을 이해한다\n",
    "5. 서브 네트워크 bottleNeckPSP과 bottleNeckIdentifyPSP을 구현할 수 있다\n",
    "6. Feature 모듈을 구현할 수 있다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 학습 목표\n",
    "\n",
    "1.\tPyramid Pooling 모듈의 서브 네트워크 구성을 이해한다\n",
    "2.\tPyramid Pooling 모듈의 멀티 스케일 처리의 구현 방법을 이해한다\n",
    "3.\tPyramid Pooling 모듈을 구현할 수 있다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6 학습 목표\n",
    "\n",
    "1.\tDecoder 모듈의 서브 네트워크 구성을 이해한다\n",
    "2.\tDecoder 모듈을 구현할 수 있다\n",
    "3.\tAuxLoss 모듈의 서브 네트워크 구성을 이해한다\n",
    "4.\tAuxLoss 모듈을 구현할 수 있다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사전 준비\n",
    "\n",
    "없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 PSPNet 네트워크 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSPNet(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(PSPNet, self).__init__()\n",
    "\n",
    "        # 파라미터 설정\n",
    "        block_config = [3, 4, 6, 3]  # resnet50\n",
    "        img_size = 475\n",
    "        img_size_8 = 60  # img_size의 1/8로 설정\n",
    "\n",
    "        # 4개의 모듈을 구성하는 서브 네트워크 준비\n",
    "        self.feature_conv = FeatureMap_convolution()\n",
    "        self.feature_res_1 = ResidualBlockPSP(\n",
    "            n_blocks=block_config[0], in_channels=128, mid_channels=64, out_channels=256, stride=1, dilation=1)\n",
    "        self.feature_res_2 = ResidualBlockPSP(\n",
    "            n_blocks=block_config[1], in_channels=256, mid_channels=128, out_channels=512, stride=2, dilation=1)\n",
    "        self.feature_dilated_res_1 = ResidualBlockPSP(\n",
    "            n_blocks=block_config[2], in_channels=512, mid_channels=256, out_channels=1024, stride=1, dilation=2)\n",
    "        self.feature_dilated_res_2 = ResidualBlockPSP(\n",
    "            n_blocks=block_config[3], in_channels=1024, mid_channels=512, out_channels=2048, stride=1, dilation=4)\n",
    "\n",
    "        self.pyramid_pooling = PyramidPooling(in_channels=2048, pool_sizes=[\n",
    "            6, 3, 2, 1], height=img_size_8, width=img_size_8)\n",
    "\n",
    "        self.decode_feature = DecodePSPFeature(\n",
    "            height=img_size, width=img_size, n_classes=n_classes)\n",
    "\n",
    "        self.aux = AuxiliaryPSPlayers(\n",
    "            in_channels=1024, height=img_size, width=img_size, n_classes=n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_conv(x)\n",
    "        x = self.feature_res_1(x)\n",
    "        x = self.feature_res_2(x)\n",
    "        x = self.feature_dilated_res_1(x)\n",
    "\n",
    "        output_aux = self.aux(x)  # Feature 모듈의 중간을 Aux 모듈로\n",
    "\n",
    "        x = self.feature_dilated_res_2(x)\n",
    "\n",
    "        x = self.pyramid_pooling(x)\n",
    "        output = self.decode_feature(x)\n",
    "\n",
    "        return (output, output_aux)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Feature 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv2DBatchNormRelu(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation, bias):\n",
    "        super(conv2DBatchNormRelu, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels,\n",
    "                              kernel_size, stride, padding, dilation, bias=bias)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # inplase 설정으로, 입력을 저장하지 않고 출력을 계산하여 메모리 절약\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.batchnorm(x)\n",
    "        outputs = self.relu(x)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureMap_convolution(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''구성할 네트워크 준비'''\n",
    "        super(FeatureMap_convolution, self).__init__()\n",
    "\n",
    "        # 합성곱 층1\n",
    "        in_channels, out_channels, kernel_size, stride, padding, dilation, bias = 3, 64, 3, 2, 1, 1, False\n",
    "        self.cbnr_1 = conv2DBatchNormRelu(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, dilation, bias)\n",
    "\n",
    "        # 합성곱 층2\n",
    "        in_channels, out_channels, kernel_size, stride, padding, dilation, bias = 64, 64, 3, 1, 1, 1, False\n",
    "        self.cbnr_2 = conv2DBatchNormRelu(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, dilation, bias)\n",
    "\n",
    "        # 합성곱 층3\n",
    "        in_channels, out_channels, kernel_size, stride, padding, dilation, bias = 64, 128, 3, 1, 1, 1, False\n",
    "        self.cbnr_3 = conv2DBatchNormRelu(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, dilation, bias)\n",
    "\n",
    "        # MaxPooling 층\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cbnr_1(x)\n",
    "        x = self.cbnr_2(x)\n",
    "        x = self.cbnr_3(x)\n",
    "        outputs = self.maxpool(x)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlockPSP(nn.Sequential):\n",
    "    def __init__(self, n_blocks, in_channels, mid_channels, out_channels, stride, dilation):\n",
    "        super(ResidualBlockPSP, self).__init__()\n",
    "\n",
    "        # bottleNeckPSP를 준비\n",
    "        self.add_module(\n",
    "            \"block1\",\n",
    "            bottleNeckPSP(in_channels, mid_channels,\n",
    "                          out_channels, stride, dilation)\n",
    "        )\n",
    "\n",
    "        # bottleNeckIdentifyPSP 반복 준비\n",
    "        for i in range(n_blocks - 1):\n",
    "            self.add_module(\n",
    "                \"block\" + str(i+2),\n",
    "                bottleNeckIdentifyPSP(\n",
    "                    out_channels, mid_channels, stride, dilation)\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv2DBatchNorm(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation, bias):\n",
    "        super(conv2DBatchNorm, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels,\n",
    "                              kernel_size, stride, padding, dilation, bias=bias)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        outputs = self.batchnorm(x)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bottleNeckPSP(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, stride, dilation):\n",
    "        super(bottleNeckPSP, self).__init__()\n",
    "\n",
    "        self.cbr_1 = conv2DBatchNormRelu(\n",
    "            in_channels, mid_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
    "        self.cbr_2 = conv2DBatchNormRelu(\n",
    "            mid_channels, mid_channels, kernel_size=3, stride=stride, padding=dilation, dilation=dilation, bias=False)\n",
    "        self.cb_3 = conv2DBatchNorm(\n",
    "            mid_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
    "\n",
    "        # 스킵 결합\n",
    "        self.cb_residual = conv2DBatchNorm(\n",
    "            in_channels, out_channels, kernel_size=1, stride=stride, padding=0, dilation=1, bias=False)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv = self.cb_3(self.cbr_2(self.cbr_1(x)))\n",
    "        residual = self.cb_residual(x)\n",
    "        return self.relu(conv + residual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bottleNeckIdentifyPSP(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, stride, dilation):\n",
    "        super(bottleNeckIdentifyPSP, self).__init__()\n",
    "\n",
    "        self.cbr_1 = conv2DBatchNormRelu(\n",
    "            in_channels, mid_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
    "        self.cbr_2 = conv2DBatchNormRelu(\n",
    "            mid_channels, mid_channels, kernel_size=3, stride=1, padding=dilation, dilation=dilation, bias=False)\n",
    "        self.cb_3 = conv2DBatchNorm(\n",
    "            mid_channels, in_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv = self.cb_3(self.cbr_2(self.cbr_1(x)))\n",
    "        residual = x\n",
    "        return self.relu(conv + residual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 Pyramid Pooling 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyramidPooling(nn.Module):\n",
    "    def __init__(self, in_channels, pool_sizes, height, width):\n",
    "        super(PyramidPooling, self).__init__()\n",
    "\n",
    "        # forward에 사용하는 화상 크기\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "        # 각 합성곱 층의 출력 채널 수\n",
    "        out_channels = int(in_channels / len(pool_sizes))\n",
    "\n",
    "        # 각 합성곱 층을 작성\n",
    "        # 다음은 for문으로 구현하는 것이 낫지만, 이해를 돕기 위해 하나하나 나열하고 있습니다\n",
    "        # pool_sizes: [6, 3, 2, 1]\n",
    "        self.avpool_1 = nn.AdaptiveAvgPool2d(output_size=pool_sizes[0])\n",
    "        self.cbr_1 = conv2DBatchNormRelu(\n",
    "            in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
    "\n",
    "        self.avpool_2 = nn.AdaptiveAvgPool2d(output_size=pool_sizes[1])\n",
    "        self.cbr_2 = conv2DBatchNormRelu(\n",
    "            in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
    "\n",
    "        self.avpool_3 = nn.AdaptiveAvgPool2d(output_size=pool_sizes[2])\n",
    "        self.cbr_3 = conv2DBatchNormRelu(\n",
    "            in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
    "\n",
    "        self.avpool_4 = nn.AdaptiveAvgPool2d(output_size=pool_sizes[3])\n",
    "        self.cbr_4 = conv2DBatchNormRelu(\n",
    "            in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out1 = self.cbr_1(self.avpool_1(x))\n",
    "        out1 = F.interpolate(out1, size=(\n",
    "            self.height, self.width), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        out2 = self.cbr_2(self.avpool_2(x))\n",
    "        out2 = F.interpolate(out2, size=(\n",
    "            self.height, self.width), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        out3 = self.cbr_3(self.avpool_3(x))\n",
    "        out3 = F.interpolate(out3, size=(\n",
    "            self.height, self.width), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        out4 = self.cbr_4(self.avpool_4(x))\n",
    "        out4 = F.interpolate(out4, size=(\n",
    "            self.height, self.width), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        # 최종적으로 결합시킬, dim=1으로 채널 수의 차원으로 결합\n",
    "        output = torch.cat([x, out1, out2, out3, out4], dim=1)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6 Decoder, AuxLoss 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodePSPFeature(nn.Module):\n",
    "    def __init__(self, height, width, n_classes):\n",
    "        super(DecodePSPFeature, self).__init__()\n",
    "\n",
    "        # forward에 사용하는 화상 크기\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "        self.cbr = conv2DBatchNormRelu(\n",
    "            in_channels=4096, out_channels=512, kernel_size=3, stride=1, padding=1, dilation=1, bias=False)\n",
    "        self.dropout = nn.Dropout2d(p=0.1)\n",
    "        self.classification = nn.Conv2d(\n",
    "            in_channels=512, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cbr(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classification(x)\n",
    "        output = F.interpolate(\n",
    "            x, size=(self.height, self.width), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxiliaryPSPlayers(nn.Module):\n",
    "    def __init__(self, in_channels, height, width, n_classes):\n",
    "        super(AuxiliaryPSPlayers, self).__init__()\n",
    "\n",
    "        # forward에 사용하는 화상 크기\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "        self.cbr = conv2DBatchNormRelu(\n",
    "            in_channels=in_channels, out_channels=256, kernel_size=3, stride=1, padding=1, dilation=1, bias=False)\n",
    "        self.dropout = nn.Dropout2d(p=0.1)\n",
    "        self.classification = nn.Conv2d(\n",
    "            in_channels=256, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cbr(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classification(x)\n",
    "        output = F.interpolate(\n",
    "            x, size=(self.height, self.width), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 동작 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PSPNet(\n",
       "  (feature_conv): FeatureMap_convolution(\n",
       "    (cbnr_1): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (cbnr_2): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (cbnr_3): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (feature_res_1): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (feature_res_2): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (block4): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (feature_dilated_res_1): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (block4): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (block5): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (block6): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (feature_dilated_res_2): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (pyramid_pooling): PyramidPooling(\n",
       "    (avpool_1): AdaptiveAvgPool2d(output_size=6)\n",
       "    (cbr_1): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (avpool_2): AdaptiveAvgPool2d(output_size=3)\n",
       "    (cbr_2): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (avpool_3): AdaptiveAvgPool2d(output_size=2)\n",
       "    (cbr_3): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (avpool_4): AdaptiveAvgPool2d(output_size=1)\n",
       "    (cbr_4): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (decode_feature): DecodePSPFeature(\n",
       "    (cbr): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(4096, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (dropout): Dropout2d(p=0.1)\n",
       "    (classification): Conv2d(512, 21, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (aux): AuxiliaryPSPlayers(\n",
       "    (cbr): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (dropout): Dropout2d(p=0.1)\n",
       "    (classification): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# 모델 정의\n",
    "net = PSPNet(n_classes=21)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(tensor([[[[ 2.3487e-01,  2.3899e-01,  2.4311e-01,  ...,  2.2102e-01,\n            2.1546e-01,  2.0990e-01],\n          [ 2.3616e-01,  2.4124e-01,  2.4632e-01,  ...,  2.1806e-01,\n            2.1378e-01,  2.0949e-01],\n          [ 2.3745e-01,  2.4349e-01,  2.4953e-01,  ...,  2.1511e-01,\n            2.1210e-01,  2.0908e-01],\n          ...,\n          [ 1.3263e-02,  6.2999e-03, -6.6358e-04,  ...,  3.8212e-02,\n            2.4333e-02,  1.0453e-02],\n          [-2.7914e-02, -3.3972e-02, -4.0030e-02,  ...,  2.5465e-02,\n            7.8839e-03, -9.6972e-03],\n          [-6.9091e-02, -7.4244e-02, -7.9396e-02,  ...,  1.2718e-02,\n           -8.5650e-03, -2.9848e-02]],\n\n         [[ 4.3609e-02,  4.2690e-02,  4.1772e-02,  ...,  4.9300e-02,\n            4.7156e-02,  4.5012e-02],\n          [ 1.2268e-02,  1.1013e-02,  9.7577e-03,  ...,  3.1725e-02,\n            3.1429e-02,  3.1132e-02],\n          [-1.9074e-02, -2.0665e-02, -2.2256e-02,  ...,  1.4150e-02,\n            1.5701e-02,  1.7252e-02],\n          ...,\n          [-2.7946e-01, -2.6510e-01, -2.5074e-01,  ..., -9.3008e-02,\n           -6.1101e-02, -2.9194e-02],\n          [-3.1799e-01, -3.0323e-01, -2.8848e-01,  ..., -1.2327e-01,\n           -8.8413e-02, -5.3561e-02],\n          [-3.5652e-01, -3.4136e-01, -3.2621e-01,  ..., -1.5352e-01,\n           -1.1572e-01, -7.7927e-02]],\n\n         [[-3.2896e-01, -3.0713e-01, -2.8530e-01,  ..., -3.2900e-01,\n           -3.1816e-01, -3.0731e-01],\n          [-3.3309e-01, -3.1069e-01, -2.8829e-01,  ..., -3.1092e-01,\n           -2.9850e-01, -2.8608e-01],\n          [-3.3721e-01, -3.1424e-01, -2.9128e-01,  ..., -2.9283e-01,\n           -2.7884e-01, -2.6484e-01],\n          ...,\n          [-2.8817e-01, -2.9847e-01, -3.0877e-01,  ..., -1.4800e-01,\n           -1.4021e-01, -1.3242e-01],\n          [-2.5594e-01, -2.7070e-01, -2.8546e-01,  ..., -1.6306e-01,\n           -1.6244e-01, -1.6181e-01],\n          [-2.2370e-01, -2.4292e-01, -2.6214e-01,  ..., -1.7813e-01,\n           -1.8466e-01, -1.9120e-01]],\n\n         ...,\n\n         [[ 4.2771e-01,  4.0424e-01,  3.8078e-01,  ...,  8.6969e-02,\n            6.5725e-02,  4.4481e-02],\n          [ 4.0688e-01,  3.8855e-01,  3.7021e-01,  ...,  1.2516e-01,\n            1.0686e-01,  8.8554e-02],\n          [ 3.8606e-01,  3.7285e-01,  3.5964e-01,  ...,  1.6336e-01,\n            1.4799e-01,  1.3263e-01],\n          ...,\n          [ 4.2900e-01,  4.4551e-01,  4.6201e-01,  ...,  6.9705e-01,\n            6.9226e-01,  6.8748e-01],\n          [ 4.4964e-01,  4.6619e-01,  4.8275e-01,  ...,  6.9410e-01,\n            6.9042e-01,  6.8673e-01],\n          [ 4.7028e-01,  4.8688e-01,  5.0348e-01,  ...,  6.9115e-01,\n            6.8857e-01,  6.8599e-01]],\n\n         [[-4.0328e-02, -3.5880e-02, -3.1431e-02,  ..., -1.7865e-02,\n           -1.9382e-02, -2.0900e-02],\n          [-8.4650e-02, -7.6106e-02, -6.7562e-02,  ..., -1.2627e-02,\n           -1.0405e-02, -8.1826e-03],\n          [-1.2897e-01, -1.1633e-01, -1.0369e-01,  ..., -7.3883e-03,\n           -1.4269e-03,  4.5345e-03],\n          ...,\n          [-1.9968e-01, -1.8671e-01, -1.7375e-01,  ...,  3.3445e-03,\n            2.9449e-02,  5.5553e-02],\n          [-2.0495e-01, -1.8626e-01, -1.6756e-01,  ..., -3.8546e-03,\n            2.5769e-02,  5.5393e-02],\n          [-2.1023e-01, -1.8580e-01, -1.6138e-01,  ..., -1.1054e-02,\n            2.2089e-02,  5.5233e-02]],\n\n         [[ 1.0118e-01,  1.0572e-01,  1.1026e-01,  ...,  1.0316e-01,\n            7.8126e-02,  5.3096e-02],\n          [ 9.7761e-02,  1.0748e-01,  1.1719e-01,  ...,  1.0617e-01,\n            8.3801e-02,  6.1428e-02],\n          [ 9.4341e-02,  1.0924e-01,  1.2413e-01,  ...,  1.0919e-01,\n            8.9476e-02,  6.9760e-02],\n          ...,\n          [-1.9137e-01, -1.5425e-01, -1.1713e-01,  ...,  7.8771e-02,\n            6.0059e-02,  4.1347e-02],\n          [-1.9371e-01, -1.5808e-01, -1.2246e-01,  ...,  4.8972e-02,\n            2.8420e-02,  7.8678e-03],\n          [-1.9605e-01, -1.6192e-01, -1.2779e-01,  ...,  1.9173e-02,\n           -3.2190e-03, -2.5611e-02]]],\n\n\n        [[[ 1.7352e-01,  1.4792e-01,  1.2233e-01,  ...,  1.9086e-01,\n            2.1332e-01,  2.3578e-01],\n          [ 1.5322e-01,  1.3027e-01,  1.0731e-01,  ...,  1.9064e-01,\n            2.1088e-01,  2.3111e-01],\n          [ 1.3293e-01,  1.1261e-01,  9.2295e-02,  ...,  1.9043e-01,\n            2.0843e-01,  2.2644e-01],\n          ...,\n          [ 7.3177e-02,  5.9325e-02,  4.5473e-02,  ..., -9.2967e-02,\n           -1.0892e-01, -1.2487e-01],\n          [ 8.7412e-02,  6.9731e-02,  5.2050e-02,  ..., -8.8606e-02,\n           -1.0809e-01, -1.2757e-01],\n          [ 1.0165e-01,  8.0137e-02,  5.8627e-02,  ..., -8.4245e-02,\n           -1.0725e-01, -1.3026e-01]],\n\n         [[ 1.0711e-01,  1.1059e-01,  1.1406e-01,  ...,  3.4286e-02,\n            4.5141e-02,  5.5996e-02],\n          [ 7.4680e-02,  8.2872e-02,  9.1063e-02,  ...,  3.8869e-02,\n            4.9942e-02,  6.1015e-02],\n          [ 4.2247e-02,  5.5157e-02,  6.8067e-02,  ...,  4.3453e-02,\n            5.4744e-02,  6.6035e-02],\n          ...,\n          [ 9.1879e-02,  6.7591e-02,  4.3304e-02,  ..., -2.7502e-02,\n           -3.1145e-02, -3.4788e-02],\n          [ 5.9597e-02,  3.7441e-02,  1.5285e-02,  ..., -1.2637e-02,\n           -1.6793e-02, -2.0950e-02],\n          [ 2.7316e-02,  7.2907e-03, -1.2735e-02,  ...,  2.2286e-03,\n           -2.4415e-03, -7.1117e-03]],\n\n         [[-3.4771e-01, -3.5817e-01, -3.6863e-01,  ..., -3.0059e-01,\n           -3.1501e-01, -3.2943e-01],\n          [-3.5736e-01, -3.6908e-01, -3.8080e-01,  ..., -2.8518e-01,\n           -2.8906e-01, -2.9295e-01],\n          [-3.6701e-01, -3.7998e-01, -3.9296e-01,  ..., -2.6977e-01,\n           -2.6312e-01, -2.5647e-01],\n          ...,\n          [-4.5555e-01, -4.4388e-01, -4.3220e-01,  ..., -2.0210e-01,\n           -1.8715e-01, -1.7221e-01],\n          [-4.5904e-01, -4.4350e-01, -4.2795e-01,  ..., -1.8551e-01,\n           -1.7082e-01, -1.5613e-01],\n          [-4.6253e-01, -4.4311e-01, -4.2369e-01,  ..., -1.6893e-01,\n           -1.5449e-01, -1.4005e-01]],\n\n         ...,\n\n         [[ 5.2669e-02,  7.1389e-02,  9.0110e-02,  ...,  1.7362e-01,\n            1.9772e-01,  2.2183e-01],\n          [ 7.1494e-02,  8.6932e-02,  1.0237e-01,  ...,  1.8571e-01,\n            2.1016e-01,  2.3460e-01],\n          [ 9.0318e-02,  1.0248e-01,  1.1463e-01,  ...,  1.9781e-01,\n            2.2259e-01,  2.4737e-01],\n          ...,\n          [ 1.7486e-01,  1.9945e-01,  2.2405e-01,  ...,  2.1761e-01,\n            2.0832e-01,  1.9902e-01],\n          [ 1.9146e-01,  2.1079e-01,  2.3012e-01,  ...,  2.3000e-01,\n            2.2277e-01,  2.1554e-01],\n          [ 2.0807e-01,  2.2213e-01,  2.3619e-01,  ...,  2.4240e-01,\n            2.3723e-01,  2.3206e-01]],\n\n         [[ 4.6293e-02,  7.3689e-02,  1.0108e-01,  ...,  2.8308e-02,\n            4.9004e-02,  6.9701e-02],\n          [ 3.4885e-02,  6.1536e-02,  8.8188e-02,  ...,  2.9808e-02,\n            5.1980e-02,  7.4152e-02],\n          [ 2.3476e-02,  4.9383e-02,  7.5291e-02,  ...,  3.1309e-02,\n            5.4956e-02,  7.8604e-02],\n          ...,\n          [-1.5351e-01, -1.3627e-01, -1.1903e-01,  ...,  1.7625e-02,\n            1.3867e-02,  1.0109e-02],\n          [-1.3847e-01, -1.2174e-01, -1.0501e-01,  ...,  1.7198e-02,\n            1.5741e-02,  1.4283e-02],\n          [-1.2342e-01, -1.0721e-01, -9.0999e-02,  ...,  1.6770e-02,\n            1.7614e-02,  1.8458e-02]],\n\n         [[ 2.3397e-01,  2.1180e-01,  1.8963e-01,  ...,  1.5467e-01,\n            1.6421e-01,  1.7375e-01],\n          [ 2.0660e-01,  1.9408e-01,  1.8156e-01,  ...,  1.9259e-01,\n            2.0452e-01,  2.1645e-01],\n          [ 1.7923e-01,  1.7636e-01,  1.7349e-01,  ...,  2.3052e-01,\n            2.4483e-01,  2.5915e-01],\n          ...,\n          [-8.1792e-02, -8.6334e-02, -9.0875e-02,  ...,  9.2913e-02,\n            8.5161e-02,  7.7408e-02],\n          [-7.9881e-02, -9.0706e-02, -1.0153e-01,  ...,  6.1009e-02,\n            5.0479e-02,  3.9949e-02],\n          [-7.7970e-02, -9.5078e-02, -1.1219e-01,  ...,  2.9105e-02,\n            1.5797e-02,  2.4902e-03]]]], grad_fn=<UpsampleBilinear2DBackward>), tensor([[[[-1.1575e-02, -4.1851e-02, -7.2127e-02,  ..., -2.0226e-01,\n           -1.9752e-01, -1.9277e-01],\n          [-2.7658e-02, -5.1706e-02, -7.5755e-02,  ..., -2.0986e-01,\n           -2.0205e-01, -1.9423e-01],\n          [-4.3740e-02, -6.1561e-02, -7.9382e-02,  ..., -2.1745e-01,\n           -2.0657e-01, -1.9570e-01],\n          ...,\n          [-4.8435e-02, -3.7863e-02, -2.7291e-02,  ..., -2.7022e-02,\n           -8.6565e-03,  9.7091e-03],\n          [-6.6996e-02, -5.2321e-02, -3.7647e-02,  ..., -1.2643e-02,\n            4.3130e-03,  2.1269e-02],\n          [-8.5558e-02, -6.6780e-02, -4.8003e-02,  ...,  1.7362e-03,\n            1.7283e-02,  3.2829e-02]],\n\n         [[ 5.1188e-02,  4.6547e-02,  4.1905e-02,  ...,  1.9596e-01,\n            1.9937e-01,  2.0277e-01],\n          [ 1.1201e-02,  1.1275e-02,  1.1349e-02,  ...,  2.0668e-01,\n            2.1328e-01,  2.1988e-01],\n          [-2.8787e-02, -2.3997e-02, -1.9206e-02,  ...,  2.1740e-01,\n            2.2720e-01,  2.3700e-01],\n          ...,\n          [ 1.2978e-01,  1.1966e-01,  1.0954e-01,  ...,  3.9742e-02,\n            3.1234e-02,  2.2725e-02],\n          [ 1.1156e-01,  1.0297e-01,  9.4370e-02,  ...,  1.7081e-02,\n           -6.7013e-04, -1.8421e-02],\n          [ 9.3354e-02,  8.6275e-02,  7.9196e-02,  ..., -5.5805e-03,\n           -3.2574e-02, -5.9568e-02]],\n\n         [[ 4.9096e-02,  5.7790e-02,  6.6485e-02,  ...,  7.2972e-02,\n            3.8085e-02,  3.1989e-03],\n          [ 3.4910e-02,  4.3499e-02,  5.2088e-02,  ...,  7.5966e-02,\n            4.2680e-02,  9.3943e-03],\n          [ 2.0724e-02,  2.9207e-02,  3.7691e-02,  ...,  7.8961e-02,\n            4.7275e-02,  1.5590e-02],\n          ...,\n          [-1.7605e-01, -1.3497e-01, -9.3892e-02,  ...,  1.3609e-01,\n            1.4853e-01,  1.6096e-01],\n          [-1.8491e-01, -1.4116e-01, -9.7405e-02,  ...,  1.2607e-01,\n            1.3857e-01,  1.5108e-01],\n          [-1.9378e-01, -1.4735e-01, -1.0092e-01,  ...,  1.1605e-01,\n            1.2862e-01,  1.4119e-01]],\n\n         ...,\n\n         [[-1.9109e-01, -1.8301e-01, -1.7493e-01,  ..., -1.8779e-01,\n           -2.2330e-01, -2.5880e-01],\n          [-1.5692e-01, -1.5413e-01, -1.5135e-01,  ..., -1.6118e-01,\n           -1.9923e-01, -2.3729e-01],\n          [-1.2275e-01, -1.2526e-01, -1.2777e-01,  ..., -1.3456e-01,\n           -1.7517e-01, -2.1577e-01],\n          ...,\n          [-1.3289e-01, -1.2798e-01, -1.2307e-01,  ..., -7.2025e-03,\n           -2.3663e-02, -4.0123e-02],\n          [-1.4257e-01, -1.3413e-01, -1.2568e-01,  ...,  1.0124e-02,\n           -1.0275e-02, -3.0674e-02],\n          [-1.5226e-01, -1.4028e-01, -1.2830e-01,  ...,  2.7451e-02,\n            3.1124e-03, -2.1226e-02]],\n\n         [[-9.2761e-02, -1.1760e-01, -1.4244e-01,  ..., -1.0369e-01,\n           -1.1582e-01, -1.2796e-01],\n          [-8.7205e-02, -1.1160e-01, -1.3599e-01,  ..., -8.2686e-02,\n           -9.5623e-02, -1.0856e-01],\n          [-8.1649e-02, -1.0559e-01, -1.2953e-01,  ..., -6.1679e-02,\n           -7.5421e-02, -8.9163e-02],\n          ...,\n          [-1.8856e-01, -1.7478e-01, -1.6100e-01,  ..., -1.6730e-01,\n           -1.5287e-01, -1.3844e-01],\n          [-1.7286e-01, -1.6062e-01, -1.4838e-01,  ..., -1.6488e-01,\n           -1.4704e-01, -1.2919e-01],\n          [-1.5715e-01, -1.4646e-01, -1.3576e-01,  ..., -1.6247e-01,\n           -1.4121e-01, -1.1994e-01]],\n\n         [[ 1.0204e-01,  1.2103e-01,  1.4002e-01,  ...,  2.4936e-01,\n            2.6865e-01,  2.8793e-01],\n          [ 7.7605e-02,  9.4649e-02,  1.1169e-01,  ...,  2.6548e-01,\n            2.8296e-01,  3.0045e-01],\n          [ 5.3166e-02,  6.8265e-02,  8.3365e-02,  ...,  2.8159e-01,\n            2.9728e-01,  3.1296e-01],\n          ...,\n          [ 3.1669e-01,  3.1377e-01,  3.1085e-01,  ...,  3.1273e-01,\n            3.2571e-01,  3.3868e-01],\n          [ 3.5494e-01,  3.4894e-01,  3.4293e-01,  ...,  2.8644e-01,\n            2.9558e-01,  3.0472e-01],\n          [ 3.9319e-01,  3.8410e-01,  3.7501e-01,  ...,  2.6015e-01,\n            2.6545e-01,  2.7075e-01]]],\n\n\n        [[[-2.4560e-01, -2.3994e-01, -2.3428e-01,  ..., -3.6490e-01,\n           -3.7360e-01, -3.8230e-01],\n          [-2.0741e-01, -2.0497e-01, -2.0253e-01,  ..., -3.3730e-01,\n           -3.4551e-01, -3.5372e-01],\n          [-1.6922e-01, -1.7000e-01, -1.7078e-01,  ..., -3.0971e-01,\n           -3.1743e-01, -3.2514e-01],\n          ...,\n          [ 8.4098e-02,  6.7032e-02,  4.9965e-02,  ..., -1.1403e-01,\n           -1.1242e-01, -1.1081e-01],\n          [ 8.1864e-02,  6.6049e-02,  5.0235e-02,  ..., -6.7994e-02,\n           -6.1384e-02, -5.4774e-02],\n          [ 7.9629e-02,  6.5067e-02,  5.0505e-02,  ..., -2.1954e-02,\n           -1.0346e-02,  1.2615e-03]],\n\n         [[-5.1568e-02, -9.0598e-02, -1.2963e-01,  ...,  8.0024e-02,\n            9.8523e-02,  1.1702e-01],\n          [-7.2518e-02, -9.9555e-02, -1.2659e-01,  ...,  8.0621e-02,\n            1.0251e-01,  1.2440e-01],\n          [-9.3469e-02, -1.0851e-01, -1.2355e-01,  ...,  8.1218e-02,\n            1.0649e-01,  1.3177e-01],\n          ...,\n          [-2.7420e-01, -2.4393e-01, -2.1367e-01,  ...,  1.6838e-04,\n            2.6034e-02,  5.1899e-02],\n          [-2.6558e-01, -2.4374e-01, -2.2189e-01,  ..., -3.6883e-02,\n           -1.7753e-02,  1.3781e-03],\n          [-2.5697e-01, -2.4354e-01, -2.3011e-01,  ..., -7.3935e-02,\n           -6.1539e-02, -4.9143e-02]],\n\n         [[-1.4596e-01, -1.4980e-01, -1.5363e-01,  ..., -6.3904e-02,\n           -4.8427e-02, -3.2950e-02],\n          [-1.4793e-01, -1.5000e-01, -1.5208e-01,  ..., -7.0411e-02,\n           -5.8267e-02, -4.6123e-02],\n          [-1.4989e-01, -1.5021e-01, -1.5053e-01,  ..., -7.6918e-02,\n           -6.8107e-02, -5.9297e-02],\n          ...,\n          [-5.0479e-02, -2.6532e-02, -2.5857e-03,  ..., -1.1017e-01,\n           -1.1297e-01, -1.1577e-01],\n          [-4.4509e-02, -2.1095e-02,  2.3196e-03,  ..., -1.2846e-01,\n           -1.3368e-01, -1.3890e-01],\n          [-3.8540e-02, -1.5657e-02,  7.2249e-03,  ..., -1.4676e-01,\n           -1.5439e-01, -1.6202e-01]],\n\n         ...,\n\n         [[-1.2121e-01, -1.1409e-01, -1.0696e-01,  ..., -4.7855e-02,\n           -2.0192e-02,  7.4725e-03],\n          [-8.9893e-02, -8.3676e-02, -7.7459e-02,  ..., -4.7755e-02,\n           -3.0104e-02, -1.2453e-02],\n          [-5.8576e-02, -5.3265e-02, -4.7953e-02,  ..., -4.7655e-02,\n           -4.0017e-02, -3.2379e-02],\n          ...,\n          [-3.5997e-02, -4.9080e-02, -6.2162e-02,  ..., -2.4962e-02,\n           -6.9150e-03,  1.1132e-02],\n          [-2.9120e-02, -4.7889e-02, -6.6657e-02,  ..., -4.2501e-02,\n           -2.1361e-02, -2.2063e-04],\n          [-2.2244e-02, -4.6698e-02, -7.1153e-02,  ..., -6.0039e-02,\n           -3.5807e-02, -1.1574e-02]],\n\n         [[-3.4953e-02, -6.9439e-02, -1.0393e-01,  ...,  9.6523e-02,\n            1.1910e-01,  1.4167e-01],\n          [-8.0537e-02, -1.0813e-01, -1.3573e-01,  ...,  7.7832e-02,\n            1.0126e-01,  1.2468e-01],\n          [-1.2612e-01, -1.4683e-01, -1.6753e-01,  ...,  5.9142e-02,\n            8.3414e-02,  1.0769e-01],\n          ...,\n          [-1.7800e-01, -1.7278e-01, -1.6756e-01,  ..., -4.5802e-02,\n           -4.1224e-02, -3.6647e-02],\n          [-2.0064e-01, -1.9506e-01, -1.8949e-01,  ..., -5.9997e-02,\n           -5.9315e-02, -5.8632e-02],\n          [-2.2328e-01, -2.1735e-01, -2.1142e-01,  ..., -7.4193e-02,\n           -7.7405e-02, -8.0617e-02]],\n\n         [[ 1.0553e-01,  8.9267e-02,  7.3003e-02,  ...,  1.8576e-01,\n            1.7583e-01,  1.6590e-01],\n          [ 9.9721e-02,  9.0171e-02,  8.0621e-02,  ...,  2.0311e-01,\n            1.9465e-01,  1.8619e-01],\n          [ 9.3911e-02,  9.1075e-02,  8.8238e-02,  ...,  2.2046e-01,\n            2.1347e-01,  2.0648e-01],\n          ...,\n          [ 1.2347e-01,  1.4847e-01,  1.7348e-01,  ...,  1.3376e-01,\n            1.3631e-01,  1.3885e-01],\n          [ 1.3698e-01,  1.6189e-01,  1.8679e-01,  ...,  1.2238e-01,\n            1.2651e-01,  1.3064e-01],\n          [ 1.5050e-01,  1.7530e-01,  2.0010e-01,  ...,  1.1099e-01,\n            1.1671e-01,  1.2243e-01]]]], grad_fn=<UpsampleBilinear2DBackward>))\n"
     ]
    }
   ],
   "source": [
    "# 더미 데이터 작성\n",
    "batch_size = 2\n",
    "dummy_img = torch.rand(batch_size, 3, 475, 475)\n",
    "\n",
    "# 계산\n",
    "outputs = net(dummy_img)\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "끝"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}